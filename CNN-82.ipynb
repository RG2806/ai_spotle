{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RG2806/ai_spotle/blob/master/CNN-82.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCh_pXZ0Zqz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "ef318fc5-8738-484e-ce82-769a5ebfde56"
      },
      "source": [
        "!git clone https://github.com/RG2806/ai_spotle.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ai_spotle'...\n",
            "remote: Enumerating objects: 4057, done.\u001b[K\n",
            "remote: Counting objects: 100% (4057/4057), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4054/4054), done.\u001b[K\n",
            "remote: Total 4057 (delta 16), reused 4030 (delta 1), pack-reused 0\n",
            "Receiving objects: 100% (4057/4057), 34.64 MiB | 23.38 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2zYZ0IRZ35w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential #Initialise our neural network model as a sequential network\n",
        "from keras.layers import Conv2D #Convolution operation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation#Applies activation function\n",
        "from keras.layers import Dropout#Prevents overfitting by randomly converting few outputs to zero\n",
        "from keras.layers import MaxPooling2D # Maxpooling function\n",
        "from keras.layers import Flatten # Converting 2D arrays into a 1D linear vector\n",
        "from keras.layers import Dense # Regular fully connected neural network\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, BatchNormalization, GlobalAveragePooling2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as k\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khmywcbkbgZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dataset_path):\n",
        "  classes = ['Fear','Sad','Happy']  #We will be dealing with seven different types of emotions.\n",
        "  data = []\n",
        "  labels =[]\n",
        "  with open(dataset_path, 'r') as file:\n",
        "      for line_no, line in enumerate(file.readlines()):\n",
        "          if 0 < line_no <= 35887:\n",
        "            line = line.split(',')\n",
        "            image_data = np.asarray([int(x) for x in line[1:]]).reshape(48,48)#Creating a list out of the string then converting it into a 2-Dimensional numpy array.\n",
        "            image_data =image_data.astype(np.float32)/255.0\n",
        "            data.append(image_data)\n",
        "            labels.append(classes.index(line[0]))\n",
        "            data.append(cv2.flip(image_data, 1))\n",
        "            labels.append(classes.index(line[0]))\n",
        "  data = np.expand_dims(data, -1)   \n",
        "  labels = to_categorical(labels, num_classes = 3)\n",
        "    \n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt_d1_J8cUU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1cc2d201-885e-4527-d4a2-c41733b5c9ad"
      },
      "source": [
        "dataset_path = \"/content/ai_spotle/aithon2020_level2_traning.csv\" \n",
        "train_data, train_labels = load_data(dataset_path)\n",
        "#train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = test_size,random_state = seed)\n",
        "\n",
        "print(\"Number of images in Training set:\", len(train_data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Training set: 21634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPgk624hfuGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc0423d5-58a6-49e2-bceb-57fd523979b2"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21634, 48, 48, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3uGfxm1cgVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=101,shuffle=True)\n",
        "#y_train = np_utils.to_categorical(y_train, 3)\n",
        "#y_test = np_utils.to_categorical(y_test, 3)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saBwXLdIdwaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64\n",
        "width, height = 48, 48\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(2*2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnzubNXee1rY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)\n",
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/input/model.h5', monitor='val_accuracy', verbose=1, save_best_only=True,mode='max')\n",
        "checkpointer2= ModelCheckpoint('/content/input2/model2.h5', monitor='val_loss', verbose=1, save_best_only=True,mode='min')\n",
        "callbacks = [lr_reducer, checkpointer,checkpointer2]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8UogCyfJQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# With Augmentation\n",
        "bs = 64\n",
        "epochs = 100\n",
        "\n",
        "aug = ImageDataGenerator(rotation_range=20, \n",
        "                         width_shift_range=0.1, \n",
        "                         height_shift_range=0.1,  \n",
        "                         fill_mode=\"nearest\")\n",
        "\n",
        "H = model.fit_generator(aug.flow(x_train, y_train, batch_size=bs),\n",
        "                        validation_data=(x_test, y_test), \n",
        "                        steps_per_epoch=len(x_train)//bs,\n",
        "                        callbacks=callbacks,\n",
        "                        shuffle=True,\n",
        "                        epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzBA5GRFfOdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9e8992f1-0abc-4532-cd1b-df43e0eedce8"
      },
      "source": [
        "scores = model.evaluate(x_test, y_test, batch_size=bs)\n",
        "print(\"Loss: \" + str(scores[0]))\n",
        "print(\"Accuracy: \" + str(scores[1]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68/68 [==============================] - 1s 16ms/step - loss: 0.4844 - accuracy: 0.8202\n",
            "Loss: 0.4843789041042328\n",
            "Accuracy: 0.8201987743377686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "447BEdpCWoap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "15a96c31-4d5b-4cd7-92a4-031ea8c03242"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "y_pred=model.predict(x_test)\n",
        "matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(metrics.classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.63      0.67      1051\n",
            "           1       0.73      0.81      0.77      1326\n",
            "           2       0.94      0.93      0.93      1950\n",
            "\n",
            "    accuracy                           0.82      4327\n",
            "   macro avg       0.80      0.79      0.79      4327\n",
            "weighted avg       0.82      0.82      0.82      4327\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knI83WZCjNuk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb8391e1-7a76-4c01-9398-4eeb24b8d00a"
      },
      "source": [
        "model.save_weights(\"/content/model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0taaw-mjrGB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "1f0d88e4-918c-48bd-eab7-e7ca42aea82d"
      },
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "def Data():\n",
        "    # images are 48x48\n",
        "    # N = 35887\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for count, filename in enumerate(os.listdir(\"/content/ai_spotle/test/0\")):\n",
        "          img = image.load_img(\"/content/ai_spotle/test/0/\"+filename, grayscale=True)\n",
        "          #show_img=image.load_img('/content/hawon.jpg', grayscale=False, target_size=(200, 200))\n",
        "          x = image.img_to_array(img)\n",
        "          x = np.array(x , dtype = np.float32)/255.0\n",
        "          X.append(x)\n",
        "          Y.append(0)\n",
        "    for count, filename in enumerate(os.listdir(\"/content/ai_spotle/test/1\")):\n",
        "          img = image.load_img(\"/content/ai_spotle/test/1/\"+filename, grayscale=True)\n",
        "          #show_img=image.load_img('/content/hawon.jpg', grayscale=False, target_size=(200, 200))\n",
        "          x = image.img_to_array(img)\n",
        "          x = np.array(x , dtype = np.float32)/255.0\n",
        "          X.append(x)\n",
        "          Y.append(1)\n",
        "    for count, filename in enumerate(os.listdir(\"/content/ai_spotle/test/2\")):\n",
        "          img = image.load_img(\"/content/ai_spotle/test/2/\"+filename, grayscale=True)\n",
        "          #show_img=image.load_img('/content/hawon.jpg', grayscale=False, target_size=(200, 200))\n",
        "          x = image.img_to_array(img)\n",
        "          x = np.array(x , dtype = np.float32)/255.0\n",
        "          X.append(x)\n",
        "          Y.append(2)\n",
        "    return X,Y\n",
        "\n",
        "X,Y=Data()\n",
        "X = np.expand_dims(X, -1)   \n",
        "Y = to_categorical(Y, num_classes = 3)\n",
        "import sklearn.metrics as metrics\n",
        "y_pred=model.predict(X)\n",
        "matrix = metrics.confusion_matrix(Y.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(matrix)\n",
        "print(metrics.classification_report(Y.argmax(axis=1), y_pred.argmax(axis=1)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 649  319   56]\n",
            " [ 187  964   96]\n",
            " [  68   75 1631]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.63      0.67      1024\n",
            "           1       0.71      0.77      0.74      1247\n",
            "           2       0.91      0.92      0.92      1774\n",
            "\n",
            "    accuracy                           0.80      4045\n",
            "   macro avg       0.78      0.78      0.78      4045\n",
            "weighted avg       0.80      0.80      0.80      4045\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}